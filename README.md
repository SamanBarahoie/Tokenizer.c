
# âœ¨ BPE Subword Tokenizer in C

A sleek and minimal implementation of **Byte Pair Encoding (BPE)** in pure C â€” designed for performance, transparency, and educational clarity. Learn how subword tokenization works under the hood, without any magic.

---

## ğŸš€ Overview

Byte Pair Encoding (BPE) is a widely-used algorithm in modern NLP pipelines (used by GPT, BERT, and others) to break down text into subword units â€” balancing vocabulary size and expressiveness.

This project demonstrates BPE from scratch in C, with:

- Clean lexical tokenization
- Vocabulary frequency counting
- Subword transformation
- Iterative BPE merges
- File-based output for inspection

---

## ğŸ›  Features

- ğŸ”¤ **Basic Tokenizer** â€” Splits input text into tokens based on whitespace and punctuation.
- ğŸ“š **Initial Vocabulary** â€” Tracks frequency of tokens using custom data structures.
- ğŸ§± **Subword Conversion** â€” Breaks each token into characters with boundary markers.
- ğŸ” **Greedy BPE Merge** â€” Repeatedly merges the most frequent adjacent subword pairs.
- ğŸŒ **Multilingual Support** â€” Fully supports **UTF-8** encoded non-Latin scripts like **Persian**, including:
  - Compound expressions: `Ù¾Ø¯ÛŒØ¯Ø§Ø±Ø´Ù†Ø§Ø³ÛŒÙ Ù‡Ø§ÛŒØ¯Ú¯Ø±ÛŒ`
  - Half-spaces and correct punctuation: `Ø¯Ø±-Ø¬Ù‡Ø§Ù†â€Œ-Ø¨ÙˆØ¯Ú¯ÛŒ`
  - Philosophical terminology and nested clauses
- ğŸ§µ **Thread-safe Hash Maps** â€” Uses `pthread` mutexes to ensure concurrency safety.
- ğŸ’¾ **Persistence** â€” Saves initial and final vocabularies to `.txt` files for review.

---

## âš™ï¸ Requirements

- **Compiler:** GCC or Clang
- **System:** Linux/macOS (or Windows WSL)
- **Libraries:** POSIX threads (`-pthread`), standard C libraries

---

## ğŸ§ª Quick Start

### 1. Clone & Build
```bash
git clone https://github.com/yourusername/bpe-tokenizer-c.git
cd bpe-tokenizer-c
gcc bpe_tokenizer.c -o bpe_tokenizer -pthread -Wall
```

### 2. Run
```bash
./bpe_tokenizer
```

### 3. Output
- `init_vocab.txt`: Raw token vocabulary
- `vocab.txt`: Final vocabulary after BPE merges

---

## ğŸ“‚ Project Structure

| File               | Description                             |
|--------------------|-----------------------------------------|
| `bpe_tokenizer.c`  | Main source file                        |
| `init_vocab.txt`   | Initial vocabulary snapshot             |
| `vocab.txt`        | Final BPE vocabulary output             |

---

## âœ¨ Sample Output

```
Original text length: 225
[INFO] Found 26 tokens
[INFO] Initial Vocabulary size: 25
...
[INFO] Vocabulary saved to 'vocab.txt'
```

---

## ğŸŒ Persian Language Support

This tokenizer is built to handle complex Persian input gracefully. For example, it can tokenize and process texts like:

> Ø§Ú¯Ø±Ú†Ù‡ Ù¾Ø¯ÛŒØ¯Ø§Ø±Ø´Ù†Ø§Ø³ÛŒÙ Ù‡Ø§ÛŒØ¯Ú¯Ø±ÛŒØŒ Ø¨Ù‡â€ŒÙ…Ø«Ø§Ø¨Ù‡Ù” ÛŒÚ© Ø±ÙˆÛŒÚ©Ø±Ø¯ Ù‡Ø±Ù…Ù†ÙˆØªÛŒÚ©ÛŒØŒ Ø¯Ø±ØµØ¯Ø¯ ØªØ¨ÛŒÛŒÙ† Ù†Ø³Ø¨ØªÙ ÙˆØ¬ÙˆØ¯ Ø¨Ø§ Ø²Ø¨Ø§Ù† Ø§Ø³Øª...

Such texts challenge typical NLP systems due to:

- **Ezafe constructions** (`Ù¾Ø¯ÛŒØ¯Ø§Ø±Ø´Ù†Ø§Ø³ÛŒÙ Ù‡Ø§ÛŒØ¯Ú¯Ø±ÛŒ`)
- **Half-spaces and punctuation** (`Ø¯Ø±-Ø¬Ù‡Ø§Ù†â€Œ-Ø¨ÙˆØ¯Ú¯ÛŒ`)
- **Philosophical vocabulary** (`Ú¯Ø´ÙˆØ¯Ú¯ÛŒ`, `Ø§ØµØ§Ù„Øª`, `Ù¾ÛŒØ´â€ŒÙÙ‡Ù…`)
- **Multi-layered syntax** and **compound verbs**

---

## ğŸ§  Why C?

C gives you full control over memory, threading, and performance â€” making this project ideal for:

- Systems-level NLP tooling
- Embedded language models
- Academic learning of tokenization fundamentals

---

## ğŸ“œ License

This project is licensed under the **MIT License**. Feel free to use, modify, and share.

---

## ğŸ¤ Contributions

Pull requests are welcome! If you find a bug or want to enhance functionality, feel free to open an issue or submit a PR.

---

## ğŸ’¬ Contact

Questions, feedback, or ideas? Reach out via GitHub Issues â€” letâ€™s build smarter tokenizers together.
```
